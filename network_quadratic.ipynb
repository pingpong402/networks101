{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pingpong402/networks101/blob/main/network_quadratic.ipynb)",
      "[![Open In Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/pingpong402/networks101/HEAD?filepath=network_quadratic.ipynb)"
     ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0N1setUeSR_"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "plt.close('all')\n",
        "\n",
        "## training data\n",
        "X = np.array([[1.6, -2.3, -1.1, 0, 3.5, 4.3]])\n",
        "Y = np.array([[-5.3, -41.8, -27.1, -14.3, -3, -4.3]])\n",
        "\n",
        "def zeros_like_dict(d):\n",
        "  return { k: zeros_like_dict(v) if isinstance(v, dict)\n",
        "        else np.zeros_like(v)\n",
        "        for k, v in d.items() }\n",
        "\n",
        "def logistic(x):\n",
        "  return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "def grad_logistic(x):\n",
        "  z = logistic(x)\n",
        "  return z*(1-z)\n",
        "\n",
        "def compute_linear_transform(x, W, b):\n",
        "  return W @ x + np.atleast_2d(b).T\n",
        "\n",
        "def compute_hidden_layer(x, p):\n",
        "  W = p['W']['hidden']\n",
        "  b = p['b']['hidden']\n",
        "  a = compute_linear_transform(x, W, b)\n",
        "  return logistic(a)\n",
        "\n",
        "def compute_output_layer(x, p):\n",
        "  W = p['W']['output']\n",
        "  b = p['b']['output']\n",
        "  return compute_linear_transform(x, W, b)\n",
        "\n",
        "def forward(x, p):\n",
        "  # if x is a single vector of shape (N,) make it a proper column vector: shape (N, 1)\n",
        "  if x.ndim < 2:\n",
        "    x = np.atleast_2d(x).T\n",
        "  h = compute_hidden_layer(x, p)\n",
        "  return compute_output_layer(h, p)\n",
        "\n",
        "def compute_loss(x, y, p):\n",
        "  if x.ndim < 2:\n",
        "    x = np.atleast_2d(x).T\n",
        "  if y.ndim < 2:\n",
        "    y = np.atleast_2d(y).T\n",
        "  N = x.shape[1]\n",
        "  prediction = forward(x, p)\n",
        "  error = (prediction - y)**2\n",
        "  return 0.5 * np.sum(error) / N\n",
        "\n",
        "def compute_grad_loss(x, y, p):\n",
        "  if x.ndim < 2:\n",
        "    x = np.atleast_2d(x).T\n",
        "  if y.ndim < 2:\n",
        "    y = np.atleast_2d(y).T\n",
        "  grad = zeros_like_dict(p)\n",
        "  N = x.shape[1]\n",
        "\n",
        "  # forward pass to get activations\n",
        "  a_hidden = compute_linear_transform(x, p['W']['hidden'], p['b']['hidden'])\n",
        "  x_hidden = logistic(a_hidden)\n",
        "  prediction = compute_output_layer(x_hidden, p)\n",
        "  error = prediction - y\n",
        "\n",
        "  # assume e,xh are the columns of error,x_hidden, ie the individual samples.\n",
        "  # the sum over all samples of the outer product e*x_h is equivalent to error @ x_hidden.T\n",
        "  grad['W']['output'] = 1/N * error @ x_hidden.T\n",
        "  grad['b']['output'] = 1/N * np.sum(error, axis=1)\n",
        "\n",
        "  # backpropagate error to hidden layer\n",
        "  hidden_layer_error = (p['W']['output'].T @ error)\n",
        "\n",
        "  grad['W']['hidden'] = 1/N * (hidden_layer_error * grad_logistic(a_hidden)) @ x.T\n",
        "  grad['b']['hidden'] = 1/N * np.sum(hidden_layer_error * grad_logistic(a_hidden), axis=1)\n",
        "  return grad\n",
        "\n",
        "def update_params(p, grad, tau):\n",
        "  for key1, sub_dict in p.items():\n",
        "    for key2, param_array in sub_dict.items():\n",
        "      p_block = param_array\n",
        "      g_block = grad[key1][key2]\n",
        "      p_block -= tau * g_block     # gradient descent step\n",
        "  return p\n",
        "\n",
        "def p2vec(p):\n",
        "  pvec = np.array([])\n",
        "  for key1, sub_dict in p.items():\n",
        "    for key2, param_array in sub_dict.items():\n",
        "      pvec = np.append(pvec, param_array.ravel())\n",
        "  return pvec\n",
        "\n",
        "def update_visualization(axes, visualization_data, redraw=False):\n",
        "  loss, p = visualization_data\n",
        "  axes[0].clear()\n",
        "  axes[1].clear()\n",
        "  axes[0].semilogy(loss)\n",
        "  axes[0].set_xlabel('Iteration')\n",
        "  axes[0].set_ylabel('Loss')\n",
        "  axes[0].set_title(f'loss: {loss[-1]:.4f}')\n",
        "\n",
        "  x_all = np.atleast_2d(np.arange(-5, 5, 0.1))\n",
        "  prediction = np.squeeze(forward(x_all, p))\n",
        "  axes[1].plot(np.squeeze(X),np.squeeze(Y),'rx', label='Training data')\n",
        "  axes[1].plot(np.squeeze(x_all), prediction, label='Learned function')\n",
        "  axes[1].set_xlim([-5,5])\n",
        "  axes[1].set_ylim([-50,50])\n",
        "  axes[1].legend()\n",
        "  if redraw:\n",
        "    clear_output(wait=True)     # reset output cell\n",
        "    display(fig)\n",
        "\n",
        "## number of hidden units\n",
        "N_hidden = 2\n",
        "\n",
        "\n",
        "## initialize parameters (Glorot)\n",
        "p = {'W': {}, 'b': {} }\n",
        "limit = np.sqrt(6 / (1 + N_hidden))   # hidden layer: n_in = 1, n_out = N_hidden\n",
        "p['W']['hidden'] = np.random.uniform(-limit, limit, size=(N_hidden, 1))\n",
        "limit = np.sqrt(6 / (N_hidden + 1))   # output layer: n_in = N_hidden, n_out = 1\n",
        "p['W']['output'] = np.random.uniform(-limit, limit, size=(1, N_hidden))\n",
        "p['b'] = {'hidden': np.zeros(N_hidden), 'output': np.zeros(1)}\n",
        "p0 = copy.deepcopy(p)\n",
        "\n",
        "num_iter = 2000\n",
        "tau = 0.02     # static descent stepsize\n",
        "\n",
        "loss = []\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "for k in range(num_iter):\n",
        "  grad = compute_grad_loss(X, Y, p)\n",
        "  p = update_params(p, grad, tau)\n",
        "  loss.append(compute_loss(X, Y, p))\n",
        "  if k % 100 == 0:\n",
        "    print(f'iter {k:04d}, loss {loss[-1]:.4f}')\n",
        "\n",
        "    ## uncomment next line for interactive visualization\n",
        "    #update_visualization(axes, (loss, p), True)\n",
        "\n",
        "\n",
        "print(f'Network with {p2vec(p).size} parameters, final loss {loss[-1]:.4f}')\n",
        "update_visualization(axes, (loss, p))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0OcPjsJ9gL2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
