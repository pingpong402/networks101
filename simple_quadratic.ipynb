{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pingpong402/networks101/blob/main/simple_quadratic.ipynb)",
      "[![Open In Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/pingpong402/networks101/HEAD?filepath=simple_quadratic.ipynb)"
     ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SedkkwWQRZPT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.close('all')\n",
        "\n",
        "## training data\n",
        "X = [1.6, -2.3, -1.1, 0, 3.5, 4.3]\n",
        "Y = [-5.3, -41.8, -27.1, -14.3, -3, -4.3]\n",
        "\n",
        "def f(x, p):\n",
        "  return p[0]*x**2 + p[1]*x + p[2]\n",
        "\n",
        "def compute_error(x, y, p):\n",
        "  e = f(x, p) - y\n",
        "  return 0.5 * e**2\n",
        "\n",
        "def compute_loss(X, Y, p):\n",
        "  N = len(X)\n",
        "  return sum([compute_error(x, y, p) for x, y in zip(X, Y)]) / N\n",
        "\n",
        "def compute_grad_error(x, y, p):\n",
        "  e = f(x, p) - y\n",
        "  return np.array([x**2*e, x*e, e])\n",
        "\n",
        "def compute_grad_loss(X, Y, p):\n",
        "  N = len(X)\n",
        "  return sum([compute_grad_error(x, y, p) for x,y in zip(X, Y)]) / N\n",
        "\n",
        "## Algorithm hyperparameters\n",
        "p0 = [1,0,0]      # initial parameters\n",
        "tau = 0.01       # gradient descent stepsize\n",
        "num_iter = 2000   # number of iterations\n",
        "p = p0\n",
        "loss = []\n",
        "## perform gradient descent\n",
        "for k in range(num_iter):\n",
        "  d = compute_grad_loss(X, Y, p)\n",
        "  p -= tau*d                       # descent step: update parameters\n",
        "  loss.append(compute_loss(X, Y, p))\n",
        "  if k%100 == 0:\n",
        "    print(f\"iter {k:04d}, loss={loss[-1]:.2f}, p=[{p[0]:.2f}, {p[1]:.2f}, {p[2]:.2f}]\")\n",
        "\n",
        "## visualize result\n",
        "print(f\"\\nFinal result: loss={loss[-1]}, p={p}\\n\")\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(121)\n",
        "plt.semilogy(loss)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'loss: {loss[-1]:.4f}')\n",
        "\n",
        "x_all = np.arange(-5, 5, 0.1)\n",
        "plt.subplot(122)\n",
        "plt.plot(X,Y,'rx', label='Training data')\n",
        "plt.plot(x_all, f(x_all, p0), label=f'function w/ initial parameters')\n",
        "plt.plot(x_all, f(x_all, p), label=f'function w/ learned parameters')\n",
        "plt.xlim([-5,5])\n",
        "plt.ylim([-50,50])\n",
        "plt.title(f'current parameters: a={p[0]:.2f}, b={p[1]:.2f}, c={p[2]:.2f}')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_c50zkOqSD2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
