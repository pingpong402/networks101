{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcHE0nprk7lv"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output, display\n",
        "from sklearn.datasets import fetch_openml\n",
        "from tqdm import tqdm\n",
        "\n",
        "plt.close('all')\n",
        "\n",
        "mnist = fetch_openml('mnist_784')\n",
        "\n",
        "def zeros_like_dict(d):\n",
        "  return { k: zeros_like_dict(v) if isinstance(v, dict)\n",
        "        else np.zeros_like(v)\n",
        "        for k, v in d.items() }\n",
        "\n",
        "def logistic(x):\n",
        "  return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "def grad_logistic(x):\n",
        "  z = logistic(x)\n",
        "  return z*(1-z)\n",
        "\n",
        "def softmax(x):\n",
        "  ## Max trick: subtract maximum value from each vector before computing exp to avoid\n",
        "  ## overflow and numerical instabilities\n",
        "  m = np.max(x, axis=0)\n",
        "  return np.exp(x-m) / np.sum(np.exp(x-m), axis=0)\n",
        "\n",
        "def compute_linear_transform(x, W, b):\n",
        "  return W @ x + np.atleast_2d(b).T\n",
        "\n",
        "def compute_hidden_layer(x, p):\n",
        "  W = p['W']['hidden']\n",
        "  b = p['b']['hidden']\n",
        "  a = compute_linear_transform(x, W, b)\n",
        "  return logistic(a)\n",
        "\n",
        "def compute_output_layer(x, p):\n",
        "  W = p['W']['output']\n",
        "  b = p['b']['output']\n",
        "  x_out = compute_linear_transform(x, W, b)\n",
        "  return softmax(x_out)\n",
        "\n",
        "def forward(x, p):\n",
        "  # if x is a single vector of shape (N,) make it a proper column vector: shape (N, 1)\n",
        "  if x.ndim < 2:\n",
        "    x = np.atleast_2d(x).T\n",
        "  h = compute_hidden_layer(x, p)\n",
        "  return compute_output_layer(h, p)\n",
        "\n",
        "\n",
        "def compute_loss(x, y, p):\n",
        "  if x.ndim < 2:\n",
        "    x = np.atleast_2d(x).T\n",
        "  if y.ndim < 2:\n",
        "    y = np.atleast_2d(y).T\n",
        "  N = x.shape[1]\n",
        "  prediction = forward(x, p)\n",
        "  crossentropy = -np.sum(y * np.log(prediction))\n",
        "  return crossentropy / N\n",
        "\n",
        "def compute_grad_loss(x, y, p):\n",
        "  if x.ndim < 2:\n",
        "    x = np.atleast_2d(x).T\n",
        "  if y.ndim < 2:\n",
        "    y = np.atleast_2d(y).T\n",
        "  grad = zeros_like_dict(p)\n",
        "  N = x.shape[1]\n",
        "\n",
        "  # Forward pass to get activations\n",
        "  a_hidden = compute_linear_transform(x, p['W']['hidden'], p['b']['hidden'])\n",
        "  x_hidden = logistic(a_hidden)\n",
        "  prediction = compute_output_layer(x_hidden, p)\n",
        "\n",
        "  # backprop through softmax\n",
        "  error = prediction - y\n",
        "\n",
        "  # Assume e,xh are the columns of error,x_hidden, ie the individual samples.\n",
        "  # The sum over all samples of the outer product e*x_h is equivalent to error @ x_hidden.T\n",
        "  grad['W']['output'] = 1/N * error @ x_hidden.T\n",
        "  grad['b']['output'] = 1/N * np.sum(error, axis=1)\n",
        "\n",
        "  # backprop to hidden layer\n",
        "  hidden_layer_error = (p['W']['output'].T @ error)\n",
        "  grad['W']['hidden'] = 1/N * (hidden_layer_error * grad_logistic(a_hidden)) @ x.T\n",
        "  grad['b']['hidden'] = 1/N * np.sum(hidden_layer_error * grad_logistic(a_hidden), axis=1)\n",
        "\n",
        "  crossentropy = -np.sum(y * np.log(prediction), axis=0)\n",
        "  accuracy = np.sum(np.argmax(prediction, axis=0) == np.argmax(y, axis=0)) / x.shape[1] * 100\n",
        "  return grad, np.sum(crossentropy) / N, accuracy\n",
        "\n",
        "def update_params(p, grad, tau):\n",
        "  for key1, sub_dict in p.items():\n",
        "    for key2, param_array in sub_dict.items():\n",
        "      p_block = param_array\n",
        "      g_block = grad[key1][key2]\n",
        "      p_block -= tau * g_block\n",
        "  return p\n",
        "\n",
        "def p2vec(p):\n",
        "  pvec = np.array([])\n",
        "  for key1, sub_dict in p.items():\n",
        "    for key2, param_array in sub_dict.items():\n",
        "      pvec = np.append(pvec, param_array.ravel())\n",
        "  return pvec\n",
        "\n",
        "def linesearch(x, y, p, grad, beta, sigma):\n",
        "  ## backtracking linesearch with Armijo condition\n",
        "  gvec = p2vec(grad)\n",
        "  alpha = 1.0\n",
        "  for m in range(100):    # max 100 backtracking steps, should never be reached in practice\n",
        "    alpha = beta**m*2.0\n",
        "    p_new = copy.deepcopy(p)\n",
        "    p_new = update_params(p_new, grad, alpha)\n",
        "    l_new = compute_loss(x, y, p_new)\n",
        "    th = sigma*alpha*np.dot(gvec,-gvec)\n",
        "    if l_new-l < th:\n",
        "      return alpha\n",
        "\n",
        "def setup_visualization():\n",
        "  fig = plt.figure(figsize=(14, 8))\n",
        "  gs_main = fig.add_gridspec(2, 2, width_ratios=[1, 2])\n",
        "  ax_loss = fig.add_subplot(gs_main[0, 0]) # left side for loss & accuracy\n",
        "  ax_acc = fig.add_subplot(gs_main[1, 0])\n",
        "  gs_sub = gs_main[:, 1].subgridspec(4, 4, wspace=0.1, hspace=0.3) # right side for 4x4 grid\n",
        "  ax_grid_dict = {}\n",
        "  for i in range(1, 17): # 1 to 16 subplots in the 4x4 grid\n",
        "      row = (i - 1) // 4\n",
        "      col = (i - 1) % 4\n",
        "      ax_grid_dict[i] = fig.add_subplot(gs_sub[row, col])\n",
        "  return fig, (ax_loss, ax_acc, ax_grid_dict)\n",
        "\n",
        "def update_visualization(fig, axes, vis_data):\n",
        "  ax_loss, ax_acc, ax_grid_dict = axes\n",
        "  loss, accuracy_train, accuracy_test, display_samples = vis_data\n",
        "  ax_loss.clear()\n",
        "  ax_loss.semilogy(loss)\n",
        "  ax_loss.set_xlabel('Iteration')\n",
        "  ax_loss.set_ylabel('Loss')\n",
        "  ax_loss.set_title(f'loss: {loss[-1]:.4f}')\n",
        "  ax_acc.clear()\n",
        "  ax_acc.plot(accuracy_train, label='train')\n",
        "  ax_acc.set_xlabel('Iteration')\n",
        "  ax_acc.set_ylabel('Accuracy')\n",
        "  if accuracy_test:\n",
        "    ax_acc.plot(accuracy_test, label='test')\n",
        "    ax_acc.set_title(f'accuracy train/test: {accuracy_train[-1]:.2f}/{accuracy_test[-1]:.2f}')\n",
        "  else:\n",
        "    ax_acc.set_title(f'accuracy train: {accuracy_train[-1]:.2f}')\n",
        "  ax_acc.legend()\n",
        "  for ax_key in ax_grid_dict:\n",
        "      ax_grid_dict[ax_key].clear()\n",
        "\n",
        "  y_predictions_for_display = forward(X[:, display_samples], p)\n",
        "  counter = 0\n",
        "  for m in range(2):\n",
        "    for n in range(4):\n",
        "      idx_input_plot = 4*2*m+n+1\n",
        "      idx_output_plot = idx_input_plot + 4\n",
        "      ax_grid_dict[idx_input_plot].imshow(X[:, display_samples[counter]].reshape(28, 28), cmap='gray')\n",
        "      ax_grid_dict[idx_input_plot].set_title(f'Input image nr {display_samples[counter]}', fontsize=8)\n",
        "      ax_grid_dict[idx_input_plot].axis('off')\n",
        "      ax_grid_dict[idx_output_plot].stem(y_predictions_for_display[:, counter])\n",
        "      color = 'g' if np.argmax(y_predictions_for_display[:, counter]) == np.argmax(Y[:, display_samples[counter]]) else 'r'\n",
        "      ax_grid_dict[idx_output_plot].set_title(f'Output: {np.argmax(y_predictions_for_display[:, counter])}', fontsize=8, color=color)\n",
        "      ax_grid_dict[idx_output_plot].set_xticks(range(10))\n",
        "      ax_grid_dict[idx_output_plot].tick_params(axis='x', labelsize=6)\n",
        "      ax_grid_dict[idx_output_plot].tick_params(axis='y', labelsize=6)\n",
        "      counter += 1\n",
        "  plt.tight_layout()\n",
        "  clear_output(wait=True)     # reset output cell\n",
        "  display(fig)\n",
        "\n",
        "\n",
        "imgs = mnist.data.to_numpy().transpose()\n",
        "X = imgs / 255.0\n",
        "## one-hot encoding of labels\n",
        "labels = np.array( [int(t) for t in mnist.target ] )\n",
        "Y = np.zeros((10, labels.size))\n",
        "Y[labels,range(labels.size)] = 1\n",
        "\n",
        "# visualize training data\n",
        "#fig = plt.figure(figsize=(14, 8))\n",
        "#for m in range(4):\n",
        "#  for n in range(6):\n",
        "#    ax = fig.add_subplot(4, 6, 6*m+n+1)\n",
        "#    ax.imshow(X[:,6*m+n].reshape(28, 28), cmap='gray')\n",
        "#    ax.set_title(f'Label: {np.argmax(Y[:,6*m+n])}')\n",
        "#plt.tight_layout()\n",
        "\n",
        "train_size = 20000      # use that many samples for training\n",
        "N_hidden = 6           # number of hidden units\n",
        "d_in = X.shape[0]\n",
        "d_out = Y.shape[0]\n",
        "\n",
        "X_test = X[:,train_size:]\n",
        "Y_test = Y[:,train_size:]\n",
        "X = X[:,:train_size]\n",
        "Y = Y[:,:train_size]\n",
        "\n",
        "\n",
        "## initialize parameters (Glorot)\n",
        "p = {'W': {}, 'b': {} }\n",
        "limit = np.sqrt(6 / (d_in + N_hidden))\n",
        "p['W']['hidden'] = np.random.uniform(-limit, limit, size=(N_hidden, d_in))\n",
        "limit = np.sqrt(6 / (N_hidden + d_out))\n",
        "p['W']['output'] = np.random.uniform(-limit, limit, size=(d_out, N_hidden))\n",
        "p['b'] = {'hidden': np.zeros(N_hidden), 'output': np.zeros(d_out)}\n",
        "\n",
        "## Optimization hyperparameters\n",
        "num_iter = 100\n",
        "tau = 0.2     # constant descent stepsize\n",
        "sigma = 0.01    # parameter for linesaearch\n",
        "beta = 0.5      # reduction factor linesearch\n",
        "\n",
        "loss = []\n",
        "accuracy_train = []\n",
        "accuracy_test = []\n",
        "display_samples = np.random.randint(0, X.shape[1], size=8)   # 8 random samples for display\n",
        "fig, axes = setup_visualization()\n",
        "\n",
        "## run gradient descent\n",
        "with tqdm(total=num_iter) as pbar:\n",
        "  for k in range(num_iter):\n",
        "    grad, l, accuracy = compute_grad_loss(X, Y, p)\n",
        "    loss.append(l)\n",
        "    accuracy_train.append(accuracy)\n",
        "\n",
        "    tau = linesearch(X, Y, p, grad, beta, sigma)   # compute optimal stepsize via lineasearch\n",
        "    p = update_params(p, grad, tau)\n",
        "\n",
        "    #prediction_test = forward(X_test, p)\n",
        "    #acc_test = np.sum(np.argmax(prediction_test, axis=0) == np.argmax(Y_test, axis=0)) / X_test.shape[1] * 100\n",
        "    #accuracy_test.append(acc_test)\n",
        "\n",
        "    pbar.update(1)\n",
        "    pbar.set_postfix({\"loss\": loss[-1]})\n",
        "    if k % 10 == 0:\n",
        "      vis_data = (loss, accuracy_train, accuracy_test, display_samples)\n",
        "      update_visualization(fig, axes, vis_data)\n",
        "\n",
        "print(f'Network with {p2vec(p).size} parameters, final loss {loss[-1]:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## verify gradient numerically\n",
        "\n",
        "def grad_numeric(x, y, p):\n",
        "  delta = 1e-4\n",
        "  grad = zeros_like_dict(p)\n",
        "\n",
        "  for key1, sub_dict in p.items():\n",
        "    for key2, param_array in sub_dict.items():\n",
        "      p_block = param_array   # references  the data in p\n",
        "      g_block = grad[key1][key2]\n",
        "      p_block_flat = p_block.reshape(-1)   # reshape() references the data\n",
        "      g_block_flat = g_block.reshape(-1)\n",
        "\n",
        "      ## estimate gradient numerically by perturbing each parameter\n",
        "      ## and computing the value of the loss function\n",
        "      for i in range(p_block_flat.size):\n",
        "        original_value = p_block_flat[i]\n",
        "        p_block_flat[i] = original_value + delta   # p_block_flat references the data, so this changes p\n",
        "        loss_p = compute_loss(x, y, p)\n",
        "        p_block_flat[i] = original_value - delta\n",
        "        loss_m = compute_loss(x, y, p)\n",
        "\n",
        "        # numerical gradient, central differences\n",
        "        g_block_flat[i] = (loss_p - loss_m) / (2 * delta)\n",
        "        p_block_flat[i] = original_value   # restore original value\n",
        "  return grad\n",
        "\n",
        "n_g = 5   # number of samples to compute gradient for\n",
        "grad, _, _ = compute_grad_loss(X[:,:n_g], Y[:,:n_g], p)\n",
        "grad_num = grad_numeric(X[:,:n_g], Y[:,:n_g], p)\n",
        "for k in grad.keys():\n",
        "  for l in grad[k].keys():\n",
        "    #print(f'grad {k} {l}\\n{grad[k][l]}')\n",
        "    #print(f'grad_numeric {k} {l}\\n{grad_num[k][l]}')\n",
        "    print(f'diff {k} {l}: {np.sum(np.abs(grad[k][l] - grad_num[k][l]))}')\n",
        "print(f'total diff: {np.sum(np.abs(p2vec(grad)-p2vec(grad_num)))}, number of elements in gradient: {p2vec(grad).size}')"
      ],
      "metadata": {
        "id": "e1bPJLZbsOsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-3C0lPxy0hy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}